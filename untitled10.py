# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WcuFfBOplTYgC8ZCQr0wx-zZIWmBhVp8
"""

# Consolidate all pip installs and spaCy model download
!python -m spacy download en_core_web_sm

import os
import sys

# Ensure src directory exists for module files
os.makedirs('src', exist_ok=True)

# Write src/utils.py
utils_content = """import re
import json
from typing import Optional

PHONE_RE = re.compile(r"(\\+?\\d{1,3}[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}")
EMAIL_RE = re.compile(r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\[a-zA-Z]{2,}")
LINK_RE = re.compile(r"https?://(?:www\\.)?\\S+\\.\\S+")

def extract_contact_info(text: str) -> dict:
    phones = PHONE_RE.findall(text)
    emails = EMAIL_RE.findall(text)
    links = LINK_RE.findall(text)
    return {
        "phone": phones[0] if phones else None,
        "email": emails[0] if emails else None,
        "links": list(set(links))
    }

def save_json(data, filename):
    with open(filename, 'w') as f:
        json.dump(data, f, indent=4)"""
with open('src/utils.py', 'w') as f:
    f.write(utils_content)

# Write src/parser.py
parser_content = """import pdfplumber
import docx
import spacy
import re

nlp = spacy.load("en_core_web_sm")

def extract_text_from_pdf(pdf_path: str) -> str:
    text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text += page.extract_text() + "\\n"
    return text

def extract_text_from_docx(docx_path: str) -> str:
    doc = docx.Document(docx_path)
    text = []
    for para in doc.paragraphs:
        text.append(para.text)
    return "\\n".join(text)

def parse_resume(file_path: str) -> dict:
    text = ""
    if file_path.endswith('.pdf'):
        text = extract_text_from_pdf(file_path)
    elif file_path.endswith('.docx'):
        text = extract_text_from_docx(file_path)
    else:
        raise ValueError("Unsupported file type. Only PDF and DOCX are supported.")

    doc = nlp(text)

    name = next((ent.text for ent in doc.ents if ent.label_ == "PERSON"), "N/A")
    email = re.search(r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\[a-zA-Z]{2,}", text)
    phone = re.search(r"(\\+?\\d{1,3}[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}", text)

    sections = {}
    current_section = None
    section_titles = ["experience", "education", "skills", "projects", "awards", "summary"]
    section_pattern = re.compile(r"\\n\\s*(" + "|".join(section_titles) + r")\\s*\\n", re.IGNORECASE)

    for line in text.split('\\n'):
        match = section_pattern.search(line)
        if match:
            current_section = match.group(1).lower()
            sections[current_section] = []
        elif current_section:
            sections[current_section].append(line.strip())

    return {
        "text": text,
        "name": name,
        "email": email.group(0) if email else "N/A",
        "phone": phone.group(0) if phone else "N/A",
        "sections": {k: "\\n".join(v) for k, v in sections.items()}
    }"""
with open('src/parser.py', 'w') as f:
    f.write(parser_content)

# Write src/skills.py
skills_content = """import re
import spacy

nlp = spacy.load("en_core_web_sm")

COMMON_SKILLS = [
    "python", "sql", "machine learning", "deep learning", "pandas", "numpy",
    "data analysis", "statistics", "regression", "classification", "nlp",
    "tensorflow", "pytorch", "scikit-learn", "visualization", "tableau",
    "aws", "azure", "gcp", "docker", "kubernetes", "java", "c++", "javascript",
    "html", "css", "react", "angular", "node.js", "databases", "mongodb",
    "excel", "power bi", "sap", "crm", "project management", "agile", "scrum",
    "communication", "teamwork", "leadership", "problem solving", "critical thinking"
]

def extract_skills(text: str) -> list[str]:
    found_skills = set()
    text_lower = text.lower()

    for skill in COMMON_SKILLS:
        if re.search(r"\\b" + re.escape(skill) + r"\\b", text_lower):
            found_skills.add(skill)

    return sorted(list(found_skills))"""
with open('src/skills.py', 'w') as f:
    f.write(skills_content)

# Write src/scoring.py
scoring_content = """import joblib
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
import os

MODEL_PATH = "models/rescore.joblib"

if not os.path.exists(MODEL_PATH):
    print(f"Warning: Model not found at {MODEL_PATH}. Creating a dummy model.")
    dummy_data = {
        'text': [
            "Experienced Python developer with machine learning skills.",
            "Entry-level position, lacks relevant experience.",
            "Strong background in data analysis and SQL.",
            "Has some experience but not a good fit for this role."
        ],
        'label': [1, 0, 1, 0]
    }
    dummy_df = pd.DataFrame(dummy_data)

    vect = TfidfVectorizer(max_features=20000, ngram_range=(1,2))
    X = vect.fit_transform(dummy_df['text'].fillna(''))
    y = dummy_df['label'].astype(int)
    model = LogisticRegression(max_iter=1000)
    model.fit(X, y)

    os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)
    joblib.dump({"vectorizer": vect, "model": model}, MODEL_PATH)

def load_scorer_model(model_path: str = MODEL_PATH):
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Scorer model not found at {model_path}. Please train the model first.")
    return joblib.load(model_path)

try:
    _scorer_model_data = load_scorer_model()
    _scorer_vectorizer = _scorer_model_data["vectorizer"]
    _scorer_model = _scorer_model_data["model"]
except FileNotFoundError as e:
    print(e)
    _scorer_vectorizer = None
    _scorer_model = None

def score_resume_text(resume_text: str) -> float:
    if not _scorer_model or not _scorer_vectorizer:
        print("Scorer model not loaded. Returning default score of 0.5.")
        return 0.5

    resume_vec = _scorer_vectorizer.transform([resume_text])
    prediction = _scorer_model.predict_proba(resume_vec)[0][1]
    return float(prediction)

def full_score(resume_text: str, job_description: str = None) -> dict:
    combined_text = resume_text
    if job_description:
        combined_text = f"{resume_text}\\n\\nJob Description: {job_description}"

    relevance_score = score_resume_text(combined_text)

    return {
        "relevance_score": relevance_score,
        "detail_scores": {
            "model_prediction": relevance_score
        }
    }"""
with open('src/scoring.py', 'w') as f:
    f.write(scoring_content)

# Write src/rewriter.py
rewriter_content = """import spacy
import random

nlp = spacy.load("en_core_web_sm")

def _simple_rewrite_sentence(sentence: str) -> str:
    doc = nlp(sentence)
    tokens = [token.text for token in doc]

    substitutions = {
        "led": ["managed", "spearheaded", "directed", "oversaw"],
        "developed": ["created", "engineered", "designed", "built"],
        "achieved": ["accomplished", "attained", "succeeded in", "reached"],
        "responsible for": ["tasked with", "in charge of", "handled", "managed"],
        "worked with": ["collaborated with", "partnered with", "engaged with"],
    }

    rewritten_tokens = []
    i = 0
    while i < len(tokens):
        found_sub = False
        for original, alternatives in substitutions.items():
            original_tokens = original.split()
            if i + len(original_tokens) <= len(tokens) and \
               [t.lower() for t in tokens[i:i+len(original_tokens)]] == [t.lower() for t in original_tokens]:
                rewritten_tokens.append(random.choice(alternatives))
                i += len(original_tokens)
                found_sub = True
                break
        if not found_sub:
            rewritten_tokens.append(tokens[i])
            i += 1
    return " ".join(rewritten_tokens)

def rewrite_experience(experience_text: str, job_description: str = None) -> str:
    sentences = nlp(experience_text).sents
    rewritten_sentences = []
    for sentence in sentences:
        rewritten_sentences.append(_simple_rewrite_sentence(str(sentence).strip()))

    return "\\n".join(rewritten_sentences)"""
with open('src/rewriter.py', 'w') as f:
    f.write(rewriter_content)

# This cell's content has been moved to src/parser.py in the setup cell (wm3Iv8ENpYSy)
# The !pip install commands are also consolidated in the setup cell.

# This cell's content has been moved to src/skills.py in the setup cell (wm3Iv8ENpYSy)

# This cell's content has been moved to src/scoring.py in the setup cell (wm3Iv8ENpYSy)

# This cell's content has been moved to src/rewriter.py in the setup cell (wm3Iv8ENpYSy)

# train_scorer.py: example training script using job descriptions + labeled good/bad resumes
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
import joblib
import os


def train(input_csv: str, out_path: str):
    df = pd.read_csv(input_csv)
    # expects columns: text, label (1 good / 0 bad)
    vect = TfidfVectorizer(max_features=20000, ngram_range=(1,2))
    X = vect.fit_transform(df['text'].fillna(''))
    y = df['label'].astype(int)
    model = LogisticRegression(max_iter=1000)
    model.fit(X, y)
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    joblib.dump({"vectorizer": vect, "model": model}, out_path)
    print("Saved model to", out_path)


if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--input', type=str, default='sample_data/job_descriptions.csv')
    parser.add_argument('--out', type=str, default='models/rescore.joblib') # Changed default output path
    # Use parse_known_args() to ignore arguments passed by the Colab kernel
    args, unknown = parser.parse_known_args()

    # Ensure the sample_data directory exists
    sample_data_dir = os.path.dirname(args.input)
    if not os.path.exists(sample_data_dir):
        os.makedirs(sample_data_dir)
        print(f"Created directory: {sample_data_dir}")

    # Create a dummy CSV file if it doesn't exist
    if not os.path.exists(args.input):
        dummy_data = {
            'text': [
                "Experienced Python developer with machine learning skills.",
                "Entry-level position, lacks relevant experience.",
                "Strong background in data analysis and SQL.",
                "Has some experience but not a good fit for this role."
            ],
            'label': [1, 0, 1, 0] # 1 for good, 0 for bad
        }
        dummy_df = pd.DataFrame(dummy_data)
        dummy_df.to_csv(args.input, index=False)
        print(f"Created dummy file: {args.input}")

    train(args.input, args.out)

import sys
sys.path.append('.') # Add current directory to path to find 'src' package

import streamlit as st
from src.parser import parse_resume
from src.skills import extract_skills
from src.scoring import full_score
from src.rewriter import rewrite_experience
from src.utils import extract_contact_info, save_json
import os


st.set_page_config(page_title="AI Resume Optimizer", layout="centered")


st.title("AI Resume Optimizer")


uploaded = st.file_uploader("Upload your resume (PDF or DOCX)", type=["pdf", "docx"])
job_desc = st.text_area("Paste the target job description (optional)")


if uploaded is not None:
    # save temp
    tmp_dir = "data/tmp"
